{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment: Simple Grid World\n",
    "class SimpleGridWorld:\n",
    "    def __init__(self, grid_size=10, goal_position=9, move_penalty=-0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_position = goal_position\n",
    "        self.move_penalty = move_penalty\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the starting state\"\"\"\n",
    "        self.state = 0  # Starting at position 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Applies the action and returns the next state, reward, and done flag\"\"\"\n",
    "        if action == 0:  # Move left\n",
    "            self.state = max(0, self.state - 1)\n",
    "        elif action == 1:  # Move right\n",
    "            self.state = min(self.grid_size - 1, self.state + 1)\n",
    "        \n",
    "        # Reward function\n",
    "        if self.state == self.goal_position:\n",
    "            reward = 1  # Goal reached\n",
    "            done = True\n",
    "        else:\n",
    "            reward = self.move_penalty  # Penalize for each move\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Q-Learning Algorithm\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros((env.grid_size, 2))  # 2 actions: 0 (left), 1 (right)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action based on epsilon-greedy strategy\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(2)  # Random action (exploration)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Best action (exploitation)\n",
    "    \n",
    "    def train(self):\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # Q-learning update rule\n",
    "                self.q_table[state, action] = self.q_table[state, action] + self.alpha * (\n",
    "                    reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state, action]\n",
    "                )\n",
    "                \n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "            rewards_per_episode.append(total_reward)\n",
    "        \n",
    "        return rewards_per_episode\n",
    "\n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SARSA Algorithm\n",
    "class SARSAAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros((env.grid_size, 2))  # 2 actions: 0 (left), 1 (right)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose an action based on epsilon-greedy strategy\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(2)  # Random action (exploration)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Best action (exploitation)\n",
    "    \n",
    "    def train(self):\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                next_action = self.choose_action(next_state)\n",
    "                \n",
    "                # SARSA update rule\n",
    "                self.q_table[state, action] = self.q_table[state, action] + self.alpha * (\n",
    "                    reward + self.gamma * self.q_table[next_state, next_action] - self.q_table[state, action]\n",
    "                )\n",
    "                \n",
    "                total_reward += reward\n",
    "                state, action = next_state, next_action\n",
    "                \n",
    "            rewards_per_episode.append(total_reward)\n",
    "        \n",
    "        return rewards_per_episode\n",
    "    \n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Policy Gradient Algorithm\n",
    "class PolicyGradientAgent:\n",
    "    def __init__(self, env, gamma=0.99, episodes=1000, learning_rate=0.01):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.episodes = episodes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize the neural network for policy\n",
    "        self.model = tf.keras.Sequential([\n",
    "            layers.Dense(24, activation='relu', input_dim=env.grid_size),  # Adjusted for input_dim\n",
    "            layers.Dense(env.grid_size, activation='softmax')  # Softmax for action probabilities\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # Convert state to one-hot encoding\n",
    "        one_hot_state = np.zeros(self.env.grid_size)\n",
    "        one_hot_state[state] = 1  # Set the corresponding position to 1\n",
    "        \n",
    "        # Reshape state to match input shape of neural network\n",
    "        one_hot_state = np.reshape(one_hot_state, [1, -1])  # (1, grid_size)\n",
    "        \n",
    "        # Get action probabilities from the model\n",
    "        probabilities = self.model(one_hot_state)\n",
    "        \n",
    "        # Sample action based on the probabilities\n",
    "        action = np.random.choice(self.env.grid_size, p=probabilities.numpy()[0])\n",
    "        return action\n",
    "    \n",
    "    def compute_discounted_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[t] + self.gamma * cumulative\n",
    "            discounted_rewards[t] = cumulative\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def train(self):\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_rewards = []\n",
    "            states = []\n",
    "            actions = []\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            discounted_rewards = self.compute_discounted_rewards(episode_rewards)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = 0\n",
    "                for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "                    one_hot_state = np.zeros(self.env.grid_size)\n",
    "                    one_hot_state[state] = 1  # One-hot encode the state\n",
    "                    one_hot_state = np.reshape(one_hot_state, [1, -1])  # (1, grid_size)\n",
    "                    \n",
    "                    probabilities = self.model(one_hot_state)\n",
    "                    action_prob = probabilities[0, action]\n",
    "                    loss = -tf.math.log(action_prob) * reward  # Policy gradient loss\n",
    "                    total_loss += loss\n",
    "                total_loss /= len(states)\n",
    "            \n",
    "            grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "            rewards_per_episode.append(sum(episode_rewards))\n",
    "        \n",
    "        return rewards_per_episode\n",
    "    \n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Deep Q-learning (DQN) Algorithm\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.001, epsilon=0.1, episodes=1000, batch_size=64, memory_size=10000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = []\n",
    "        \n",
    "        # Initialize Q-network and target network\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.alpha)\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Dense(24, activation='relu', input_shape=(self.env.grid_size,)),  # Adjusted for one-hot encoding\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.env.grid_size, activation='linear')  # Output layer for Q-values\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # One-hot encode the state\n",
    "        one_hot_state = np.zeros(self.env.grid_size)\n",
    "        one_hot_state[state] = 1  # Set the corresponding position to 1\n",
    "        \n",
    "        # Reshape state to match input shape of neural network\n",
    "        one_hot_state = np.reshape(one_hot_state, [1, -1])  # (1, grid_size)\n",
    "        \n",
    "        # Get Q-values from the model\n",
    "        q_values = self.model(one_hot_state)\n",
    "        \n",
    "        # Choose the action with the highest Q-value (greedy policy)\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        batch_data = [self.memory[i] for i in batch]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch_data)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), np.array(dones)  # Ensure dones is an array\n",
    "    \n",
    "    def train(self):\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                self.store_experience(state, action, reward, next_state, done)\n",
    "                \n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    states, actions, rewards_batch, next_states, dones = self.sample_batch()\n",
    "                    \n",
    "                    # One-hot encode states and next states\n",
    "                    one_hot_states = np.zeros((self.batch_size, self.env.grid_size))\n",
    "                    one_hot_next_states = np.zeros((self.batch_size, self.env.grid_size))\n",
    "                    \n",
    "                    for i, state in enumerate(states):\n",
    "                        one_hot_states[i, state] = 1  # One-hot encode the state\n",
    "                    for i, next_state in enumerate(next_states):\n",
    "                        one_hot_next_states[i, next_state] = 1  # One-hot encode the next state\n",
    "                    \n",
    "                    # Q-learning update rule\n",
    "                    next_q_values = self.target_model(one_hot_next_states)\n",
    "                    target_q_values = rewards_batch + self.gamma * np.max(next_q_values, axis=1) * (1 - dones)\n",
    "                    \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        q_values = self.model(one_hot_states)\n",
    "                        q_value = tf.gather(q_values, actions, axis=1, batch_dims=1)\n",
    "                        loss = tf.reduce_mean((target_q_values - q_value) ** 2)\n",
    "                    \n",
    "                    grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "                    \n",
    "                    # Periodically update target model\n",
    "                    if episode % 10 == 0:\n",
    "                        self.target_model.set_weights(self.model.get_weights())\n",
    "                \n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "            rewards_per_episode.append(total_reward)\n",
    "        \n",
    "        return rewards_per_episode\n",
    "    \n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Main execution: Initialize environment and agents\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = SimpleGridWorld()\n",
    "\n",
    "    # Initialize agents\n",
    "    agents = {\n",
    "        \"Q-Learning\": QLearningAgent(env, episodes=200),\n",
    "        \"SARSA\": SARSAAgent(env, episodes=200),\n",
    "        \"Policy Gradient\": PolicyGradientAgent(env, episodes=200),\n",
    "        \"DQN\": DQNAgent(env, episodes=200)\n",
    "    }\n",
    "\n",
    "    rewards = {}\n",
    "\n",
    "    # Train each agent and store their rewards\n",
    "    for agent_name, agent in agents.items():\n",
    "        rewards[agent_name] = agent.train()\n",
    "\n",
    "    # Plot the cumulative rewards for all agents on the same graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for agent_name, agent_rewards in rewards.items():\n",
    "        plt.plot(agent_rewards, label=agent_name)\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Comparison of Reinforcement Learning Algorithms \\nKshitiz Bhargava 21BCE2067')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Kshitiz Bhargava 21BCE2067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
